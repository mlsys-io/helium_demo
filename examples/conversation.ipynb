{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "from helium import helium, ops\n",
    "from helium.graphs import Graph\n",
    "from helium.runtime import HeliumServerConfig\n",
    "\n",
    "# Use local Helium server\n",
    "server_config = HeliumServerConfig(\n",
    "    is_local=True, llm_service_configs=\"../configs/llm_services.json\"\n",
    ")\n",
    "\n",
    "# Use remote Helium server\n",
    "# In this case, you need to start a Helium server in a separate process.\n",
    "# See scripts/serve_helium.sh for an example.\n",
    "# server_config = HeliumServerConfig(is_local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Helium server locally\n",
    "helium.get_started_instance(config=server_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Writing Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_writing_simulation_graph(num_generations: int) -> Graph:\n",
    "    \"\"\"Builds a compute graph for a writing simulation task.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_generations : int\n",
    "        The number of generations to perform in the simulation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Graph\n",
    "        A compute graph that simulates a writing task where a user writes an introduction,\n",
    "        receives feedback, and revises their writing based on the feedback.\n",
    "    \"\"\"\n",
    "    # 1. Generate a draft introduction.\n",
    "    draft_instr = ops.message_data(\n",
    "        [\n",
    "            ops.OpMessage(\n",
    "                role=\"system\",\n",
    "                content=(\n",
    "                    \"You are a non-native English speaker learning English. \"\n",
    "                    \"You are to respond in basic and sometimes broken English.\"\n",
    "                ),\n",
    "            ),\n",
    "            ops.OpMessage(\n",
    "                role=\"user\",\n",
    "                content=[\"Write a paragraph to introduce yourself.\"] * num_generations,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    draft_history = ops.llm_chat(draft_instr, return_history=True)\n",
    "\n",
    "    # 2. Review the draft.\n",
    "    draft = ops.get_last_message(draft_history)\n",
    "    review_instr = ops.message_data(\n",
    "        [\n",
    "            ops.OpMessage(\n",
    "                role=\"system\",\n",
    "                content=(\n",
    "                    \"You are an English teacher. You evaluate the user's writing \"\n",
    "                    \"critically and respond with your suggestions for improvement.\"\n",
    "                ),\n",
    "            ),\n",
    "            ops.OpMessage(role=\"user\", content=draft),\n",
    "        ]\n",
    "    )\n",
    "    review = ops.llm_chat(review_instr, return_history=False)\n",
    "\n",
    "    # 3. Revise the draft based on the review.\n",
    "    revise_msg = ops.format_op(\n",
    "        \"Below is the comment on your writing. Please revise your introduction \"\n",
    "        \"accordingly and answer me with only that revised version:\\n\\n{review}\",\n",
    "        review=review,\n",
    "    )\n",
    "    revise_instr = ops.append_message(\n",
    "        draft_history, ops.OpMessage(role=\"user\", content=revise_msg)\n",
    "    )\n",
    "    final = ops.llm_chat(revise_instr, return_history=False)\n",
    "\n",
    "    # 4. Collect outputs.\n",
    "    draft_out = ops.as_output(\"draft\", draft)\n",
    "    review_out = ops.as_output(\"review\", review)\n",
    "    final_out = ops.as_output(\"final\", final)\n",
    "\n",
    "    return Graph.from_ops([draft_out, review_out, final_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the writing simulation graph\n",
    "graph = build_writing_simulation_graph(num_generations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "_ = graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the graph on the Helium server\n",
    "result = helium.invoke(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiagent Debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multiagent_debate_graph(\n",
    "    num_agents: int,\n",
    "    num_rounds: int,\n",
    ") -> Graph:\n",
    "    system_prompt = \"You are a helpful AI Assistant.\"\n",
    "    input_op = ops.input_placeholder(\"questions\")\n",
    "    revise_prompts = (\n",
    "        (\n",
    "            \"Can you double check that your answer is correct. Put your final answer \"\n",
    "            \"in the form (X) at the end of your response.\"\n",
    "        ),\n",
    "        (\n",
    "            \"Using the reasoning from other agents as additional advice, can you give \"\n",
    "            \"an updated answer? Examine your solution and that other agents step by \"\n",
    "            \"step. Put your answer in the form (X) at the end of your response.\"\n",
    "        ),\n",
    "    )\n",
    "    generation_config = None\n",
    "\n",
    "    # First round\n",
    "    initial_message_list = [\n",
    "        [\n",
    "            ops.OpMessage(role=\"system\", content=system_prompt),\n",
    "            ops.OpMessage(role=\"user\", content=input_op),\n",
    "        ]\n",
    "        for _ in range(num_agents)\n",
    "    ]\n",
    "    history_list = [\n",
    "        ops.llm_chat(message, generation_config, return_history=True)\n",
    "        for message in initial_message_list\n",
    "    ]\n",
    "\n",
    "    if num_rounds == 1:\n",
    "        return Graph.from_ops(\n",
    "            [\n",
    "                ops.as_output(f\"agent_{i}\", history)\n",
    "                for i, history in enumerate(history_list)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Debate rounds\n",
    "    if num_agents == 1:\n",
    "        revise_prompt = ops.data(revise_prompts[0])\n",
    "        new_convo_list = [\n",
    "            ops.append_message(history, revise_prompt) for history in history_list\n",
    "        ]\n",
    "    else:\n",
    "        last_message_list = [ops.get_last_message(history) for history in history_list]\n",
    "        new_convo_list = []\n",
    "        for i, history in enumerate(history_list):\n",
    "            other_agent_answers = last_message_list[:i] + last_message_list[i + 1 :]\n",
    "            revise_prompt = ops.format_op(\n",
    "                \"\\n\\n \".join(\n",
    "                    [\n",
    "                        \"These are the solutions to the problem from other agents: \",\n",
    "                        *[\n",
    "                            f\"One agent solution: ```{{agent_{j}}}```\"\n",
    "                            for j in range(num_agents - 1)\n",
    "                        ],\n",
    "                        revise_prompts[1],\n",
    "                    ]\n",
    "                ),\n",
    "                **{f\"agent_{j}\": ans for j, ans in enumerate(other_agent_answers)},\n",
    "            )\n",
    "            new_convo_list.append(ops.append_message(history, revise_prompt))\n",
    "    revised_history_list = [\n",
    "        ops.llm_chat(convo, generation_config, return_history=True)\n",
    "        for convo in new_convo_list\n",
    "    ]\n",
    "    debate_loop = ops.loop(history_list, revised_history_list, num_rounds - 1)\n",
    "\n",
    "    return Graph.from_ops(\n",
    "        [\n",
    "            ops.as_output(f\"agent_{i}\", agent_history)\n",
    "            for i, agent_history in enumerate(debate_loop)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the writing simulation graph\n",
    "graph = build_multiagent_debate_graph(num_agents=3, num_rounds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "plt.figure(figsize=(15, 15))\n",
    "_ = graph.visualize(layout=\"spring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graph with input questions\n",
    "compiled_graph = graph.compile(\n",
    "    questions=[\n",
    "        \"Can you answer the following question as accurately as possible? \"\n",
    "        \"You suspect that your patient has an enlarged submandibular salivary gland. \"\n",
    "        \"You expect the enlarged gland: \\n \"\n",
    "        \"A) to be palpable intraorally.. \\n \"\n",
    "        \"B) to be palpable extraorally. \\n \"\n",
    "        \"C) to be palpable both intra- and extraorally. \\n \"\n",
    "        \"D) only to be detectable by radiographical examination. \\n \"\n",
    "        \"Explain your answer, putting the answer in the form (X) at the end of your response.\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the graph on the Helium server\n",
    "result = helium.invoke(compiled_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "pprint.pprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
