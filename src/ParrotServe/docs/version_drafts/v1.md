# V1

This is the draft for Parrot V1.

## Executor: Executing Semantic Function

A function call will first be submitted to the dispatcher and dispatched to certain registered engine.

Then the tokenizer will be automatically selected to tokenize the text. (Cached here).

**~~NOTE: The generation task need a first token. Hence, we directly raise error if the input/output loc is adjacent to an output loc.~~**

Sampling params: auto config?

Start a session in the executor. A session contains a job queue.  instruction queue.

```
Instruction:
    - PrefixFill
    - ConstantFill
    - PlaceholderFill
    - PlaceholderGeneration
    - TextFill
    - TextGeneration
```

Sending Requests:

- Popping jobs from sessions’ queue (which are marked as ready), add them into running queue
- Popping some FillJob for merging
- Popping some FillJob for pipelining
- Send requests (this will merge some FillJobs into a single Fill request)

### Executor in Parrot

The executor of the semantic program optimizer *Parrot* is responsible for 

- Manage sessions created by semantic funtion calls.
- Submit requests to backend server.
- Collect executed results from backend.

The main characteristics are:

- Variable-level asynchronization.
- Token-level pipelining message passing through tunnel.

### Motivation

**Asynchronous is important in semantic programming.** Similar to network programming:

- We send requests to certain servers to get resources we need.
- Waiting for requests may take a long time. (Latency)
- The result is not reliable. (404, Retry)

Asynchronous programming is popular in network programming (JavaScript, Node, Python) because it lets the procedure of waiting for requests to be non-blocking. We point out that this is 
also very important in semantic programming.

**Batching is important in LLM inference**. Existing LLM Agent SDK only supports
request-level asynchronization mainly because the constraint of API, resulting small batches 
and low GPU efficiency. Sub-sequence asynchronization is more friendly to LLM execution, bringing us a larger batch in local serving scenario.

## ~~Pipe~~

![](../images/v1-token-pipe.png)


## Orchestration

The orchestration part gives us a high-level state of LLMs.

- register engine
- register function (caching prefix in some engine)
- manage contexts (two types KV cache: huggingface [high-level side] and local KV cache [engine-side]).


## Protocols

- OpenAI standard APIs
- Fill: session_id, context_id, parent_context_id, tokens_id
    - If context_id doesn’t exist in engine, fork a new context from parent_context_id.
    - Response:
- PipedFill: session_id, context_id,  parent_context_id, producer_request_id
- Generation: session_id, context_id, temperature, top_k, top_p, length_penalty,
    - Response:
- Free Context: context_id (Will not free the content in the parent context).
    - Response:
- Heartbeat:
    - Response: model_ready: bool, cached_tokens: int, running_jobs: int


## Backend

Frontend will submit several requests to backend.

**Type of Backend:**

- Native (Can understand Fill, Generation, etc. Adopts paged memory management.)
- HuggingFace (Can maintain a past key-value tensors in a context. Need max_seq_len.)
- OpenAI (Like GPT-4)
- MLCLLM

**Server Side:**

- A request maybe a Fill, Generation, FreeContext or Heartbeat.
- For FreeContext and Heartbeat, the server will execute them immediately.
- For Fill and Generation, the requests will be put in the scheduler.

**Scheduler Side:**

- The scheduler will batch different requests (including Fill and Generation).
- In one iteration, the scheduler will send tokens and input metadata to model.
- (For now, we don’t support distributed inference).

**Model Side:**

- Executing a batch of Fill & Generation requests.
- Major changes in model adaption: using paged attention (or other attention mechanism supported paged memory management) in Attention layer.

